---
---
@article{yang2025recontextualizing,
 abbr={EMNLP},
 title={Recontextualizing Revitalization: A Mixed Media Approach to Reviving the Nüshu Language},
 author={Yang, Ivory and Guo, Xiaobo, and Wang, Yuxin and Zhang, Hefan and Jia, Yaning and Dinaeur, William and Vosoughi, Soroush},
 abstract={},
 journal={EMNLP (main conference)},
 year={2025},
 ARXIV={},
 selected={true}
}

@article{ma2025enhancing,
 abbr={EMNLP},
 title={Enhancing LLM-Based Persuasion Simulations with Cultural and Speaker-Specific Information},
 author={Ma, Weicheng and Zhang, Hefan and Ji, Shiyu and Hashemi, Farnoosh and Wang, Qichao and Yang, Ivory and Chen, Joice and Pan, Juanwen and Macy, Michael and Hassanpour, Saeed and Vosoughi, Soroush},
 abstract={},
 journal={EMNLP (findings)},
 year={2025},
 ARXIV={},
 selected={false}
}

@article{yang2025visibility,
 abbr={ACL},
 title={Visibility as Survival: Generalizing NLP for Native Alaskan Language Identification},
 author={Yang, Ivory and Zhang, Chunhui and Wang, Yuxin and Ouyang, Zhongyu and Vosoughi, Soroush},
 abstract={Indigenous languages remain largely invisible in commercial language identification (LID) systems, a stark reality exemplified by Google Translate’s LangID tool, which supports over 100 languages but excludes all 150 Indigenous languages of North America. This technological marginalization is particularly acute for Alaska’s 20 Native languages, all of which face endangerment despite their rich linguistic heritage. We present GenAlaskan, a framework demonstrating how both large language models and specialized classifiers can effectively identify these languages with minimal data. Working closely with Native Alaskan community members, we create Akutaq-2k, a carefully curated dataset of 2000 sentences spanning all 20 languages, named after the traditional Yup’ik dessert, symbolizing the blending of diverse elements. We design few-shot prompting on proprietary and open-source LLMs, achieving nearly perfect accuracy with just 40 examples per language. While initial zero-shot attempts show limited success, our systematic attention head pruning revealed critical architectural components for accurate language differentiation, providing insights into model decision-making for low-resource languages. Our results challenge the notion that effective Indigenous language identification requires massive resources or corporate infrastructure, demonstrating that targeted technological interventions can drive meaningful progress in preserving endangered languages in the digital age.},
 journal={ACL (findings)},
 year={2025},
 ARXIV={https://aclanthology.org/2025.findings-acl.364/},
 selected={true}
}

@article{yang2025isitnavajo,
 abbr={NAACL},
 title={Is it Navajo? Accurate Language Detection in Endangered Athabaskan Languages},
 author={Yang, Ivory and Ma, Weicheng and Zhang, Chunhui and Vosoughi, Soroush},
 abstract={Endangered languages, such as Navajo—the most widely spoken Native American language—are significantly underrepresented in contemporary language technologies, exacerbating the challenges of their preservation and revitalization. This study evaluates Google’s Language Identification (LangID) tool, which does not currently support any Native American languages. To address this, we introduce a random forest classifier trained on Navajo and twenty erroneously suggested languages by LangID. Despite its simplicity, the classifier achieves near-perfect accuracy (97-100%). Additionally, the model demonstrates robustness across other Athabaskan languages—a family of Native American languages spoken primarily in Alaska, the Pacific Northwest, and parts of the Southwestern United States—suggesting its potential for broader application. Our findings underscore the pressing need for NLP systems that prioritize linguistic diversity and adaptability over centralized, one-size-fits-all solutions, especially in supporting underrepresented languages in a multicultural world. This work directly contributes to ongoing efforts to address cultural biases in language models and advocates for the development of culturally localized NLP tools that serve diverse linguistic communities.},
 journal={NAACL (main conference, oral)},
 year={2025},
 ARXIV={https://aclanthology.org/2025.naacl-short.24/},
 selected={true}
}

@article{ma2025persuasion,
 abbr={NAACL},
 title={Communication is All You Need: Persuasion Dataset Construction via Multi-LLM Communication},
 author={Ma, Weicheng and Zhang, Hefan and Yang, Ivory and Ji, Shiyu and Chen, Joice and Hashemi, Farnoosh and Mohole, Shubham and Gearey, Ethan and Macy, Michael, and Hassanpour, Saeed Hassanpour and Vosoughi, Soroush},
 abstract={Large Language Models (LLMs) have shown proficiency in generating persuasive dialogue, yet concerns about the fluency and sophistication of their outputs persist. This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically. This framework facilitates the efficient production of high-quality, diverse linguistic content with minimal human oversight. Through extensive evaluations, we demonstrate that the generated data excels in naturalness, linguistic diversity, and the strategic use of persuasion, even in complex scenarios involving social taboos. The framework also proves adept at generalizing across novel contexts. Our results highlight the framework’s potential to significantly advance research in both computational and social science domains concerning persuasive communication.},
 journal={NAACL (main conference, oral)},
 year={2025},
 ARXIV={https://aclanthology.org/2025.naacl-long.203/},
 selected={false}
}

@article{yang2024n,
  abbr={COLING},
  title={NüshuRescue: Revitalization of the Endangered Nüshu Language with AI},
  author={Yang, Ivory and Ma, Weicheng and Vosoughi, Soroush},
  abstract={The preservation and revitalization of endangered and extinct languages is a meaningful endeavor, conserving cultural heritage while enriching fields like linguistics and anthropology. However, these languages are typically low-resource, making their reconstruction labor-intensive and costly. This challenge is exemplified by Nüshu, a rare script historically used by Yao women in China for self-expression within a patriarchal society. To address this challenge, we introduce NüshuRescue, an AI-driven framework designed to train large language models (LLMs) on endangered languages with minimal data. NüshuRescue automates evaluation and expands target corpora to accelerate linguistic revitalization. As a foundational component, we developed NCGold, a 500-sentence Nüshu-Chinese parallel corpus, the first publicly available dataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to Nüshu and only 35 short examples from NCGold, NüshuRescue achieved 48.69\% translation accuracy on 50 withheld sentences and generated NCSilver, a set of 98 newly translated modern Chinese sentences of varying lengths. A sample of both NCGold and NCSilver is included in the Supplementary Materials. Additionally, we developed FastText-based and Seq2Seq models to further support research on Nüshu. NüshuRescue provides a versatile and scalable tool for the revitalization of endangered languages, minimizing the need for extensive human input.},
  journal={COLING (main conference, oral)},
  year={2025},
  ARXIV={2412.00218},
  selected={true}
}

@article{wang2024mentalmanip,
  abbr={ACL},
  title={MentalManip: A Dataset For Fine-grained Analysis of Mental Manipulation in Conversations},
  author={Wang, Yuxin and Yang, Ivory and Hassanpour, Saeed and Vosoughi, Soroush},
  abstract={Mental manipulation, a significant form of abuse in interpersonal conversations, presents a challenge to identify due to its context-dependent and often subtle nature. The detection of manipulative language is essential for protecting potential victims, yet the field of Natural Language Processing (NLP) currently faces a scarcity of resources and research on this topic. Our study addresses this gap by introducing a new dataset, named MentalManip, which consists of 4,000 annotated movie dialogues. This dataset enables a comprehensive analysis of mental manipulation, pinpointing both the techniques utilized for manipulation and the vulnerabilities targeted in victims. Our research further explores the effectiveness of leading-edge models in recognizing manipulative dialogue and its components through a series of experiments with various configurations. The results demonstrate that these models inadequately identify and categorize manipulative content. Attempts to improve their performance by fine-tuning with existing datasets on mental health and toxicity have not overcome these limitations. We anticipate that MentalManip will stimulate further research, leading to progress in both understanding and mitigating the impact of mental manipulation in conversations.},
  journal={ACL (main conference, oral)},
  year={2024},
  ARXIV={2405.16584},
  dataset={https://github.com/audreycs/MentalManip},
  selected={false}
}

@article{yang2024enhanced,
  abbr={EMNLP},
  title={Enhanced Detection of Conversational Mental Manipulation Through Advanced Prompting Techniques},
  author={Yang, Ivory and Guo, Xiaobo and Xie, Sean and Vosoughi, Soroush},
  abstract={This study presents a comprehensive, long-term project to explore the effectiveness of various prompting techniques in detecting dialogical mental manipulation. We implement Chain-of-Thought prompting with Zero-Shot and Few-Shot settings on a binary mental manipulation detection task, building upon existing work conducted with Zero-Shot and Few- Shot prompting. Our primary objective is to decipher why certain prompting techniques display superior performance, so as to craft a novel framework tailored for detection of mental manipulation. Preliminary findings suggest that advanced prompting techniques may not be suitable for more complex models, if they are not trained through example-based learning.},
  journal={EMNLP (WiNLP)},
  year={2024},
  ARXIV={2408.07676},
  selected={false}
}
